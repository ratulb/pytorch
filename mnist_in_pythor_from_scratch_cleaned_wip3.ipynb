{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratulb/pytorch/blob/main/mnist_in_pythor_from_scratch_cleaned_wip3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "\n",
        "\n",
        "# Define a transform to convert the data to tensor\n",
        "random.seed(24)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(24)\n",
        "\n",
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "# Load the MNIST dataset"
      ],
      "metadata": {
        "id": "l19KmJV14sJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the training and test datasets\n",
        "train_dataset = torchvision.datasets.MNIST(root='./', train=True, download=True, transform=to_tensor)\n",
        "#test_dataset = torchvision.datasets.MNIST(root='./', train=False, download=True, transform=to_tensor)\n",
        "train_ds, val_ds = random_split(train_dataset, [59000, 1000])\n",
        "len(train_ds), len(val_ds)\n",
        "#len(train_dataset)"
      ],
      "metadata": {
        "id": "ko3R62h8FxV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8VP6d9Ih3z5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ],
      "metadata": {
        "id": "9vpvDXVpAqxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_list(dataset):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for img, label in dataset:\n",
        "        # Convert the tensor image to a list\n",
        "        img_list = img.squeeze().tolist()\n",
        "        images.append(img_list)\n",
        "        labels.append(label)\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "Vi0ygacyGSmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(values, num_classes):\n",
        "    \"\"\"\n",
        "    Convert a list of values into one-hot encoded format.\n",
        "    Args:\n",
        "        values (list of list): List containing the values to encode.\n",
        "        num_classes (int): Number of classes for one-hot encoding.\n",
        "    Returns:\n",
        "        list of list: One-hot encoded representation of the input values.\n",
        "    \"\"\"\n",
        "    one_hot_encoded = []\n",
        "    for value in values:\n",
        "        one_hot = [0] * num_classes\n",
        "        one_hot[value[0]] = 1\n",
        "        one_hot_encoded.append(one_hot)\n",
        "    return one_hot_encoded\n",
        "\n",
        "#values = [[1], [0], [2], [3]]\n",
        "#num_classes = 4\n",
        "\n",
        "#one_hot_encoded_values = one_hot_encode(values, num_classes)\n",
        "#print(one_hot_encoded_values)\n"
      ],
      "metadata": {
        "id": "xT-yzmzgDVfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(input_batch, all_neurons_weights, biases):\n",
        "    batch_output = []\n",
        "    for inputs in input_batch:\n",
        "        layer_output = []\n",
        "        for weights, bias in zip(all_neurons_weights, biases):\n",
        "            neuron_output = sum(x * w for x, w in zip(inputs, weights)) + bias\n",
        "            layer_output.append(neuron_output)\n",
        "        batch_output.append(layer_output)\n",
        "    return batch_output\n",
        "\n",
        "def calculate_errors(batch_output_of_forward_pass, target_batch):\n",
        "    squared_errors = []\n",
        "    residual_errors = []\n",
        "    for output, target in zip(batch_output_of_forward_pass, target_batch):\n",
        "        try:\n",
        "            sample_squared_errors = [(o - t) ** 2 for o, t in zip(output, target)]\n",
        "        except OverflowError as e:\n",
        "            print(\"Overflow error.\", e)\n",
        "            sample_squared_errors = 0\n",
        "        #sample_squared_errors = [(o - t) ** 2 for o, t in zip(output, target)]\n",
        "        sample_residual_errors = [(o - t) for o, t in zip(output, target)]\n",
        "        squared_errors.append(sample_squared_errors)\n",
        "        residual_errors.append(sample_residual_errors)\n",
        "    return squared_errors, residual_errors\n",
        "\n",
        "def calculate_weight_and_bias_deltas(output_of_calculate_errors, input_batch, learning_rate):\n",
        "    residual_errors = output_of_calculate_errors[1]\n",
        "    weight_deltas = [[0 for _ in range(len(input_batch[0]))] for _ in range(len(residual_errors[0]))]\n",
        "    bias_deltas = [0 for _ in range(len(residual_errors[0]))]\n",
        "\n",
        "    for residual_error, inputs in zip(residual_errors, input_batch):\n",
        "        for neuron_index in range(len(residual_error)):\n",
        "            for feature_index in range(len(inputs)):\n",
        "                weight_deltas[neuron_index][feature_index] += learning_rate * residual_error[neuron_index] * inputs[feature_index]\n",
        "            bias_deltas[neuron_index] += learning_rate * residual_error[neuron_index]\n",
        "    return weight_deltas, bias_deltas\n",
        "\n",
        "def update_weights_biases(output_of_calculate_weight_and_bias_deltas, weights, biases):\n",
        "    weight_deltas, bias_deltas = output_of_calculate_weight_and_bias_deltas\n",
        "    for i in range(len(weights)):\n",
        "        for j in range(len(weights[i])):\n",
        "            weights[i][j] -= weight_deltas[i][j]\n",
        "        biases[i] -= bias_deltas[i]\n",
        "    return weights, biases\n",
        "\n",
        "def batch_validation(weights, biases, batch):\n",
        "    images, labels = batch\n",
        "    images = [image.squeeze().tolist() for image in images]\n",
        "    images = [[pixel for row in image for pixel in row] for image in images]\n",
        "    outputs = forward_pass(images, weights, biases)\n",
        "    outputs = torch.tensor([np.argmax(softmax(output)) for output in outputs])\n",
        "    accuracy_and_counts = accuracy(outputs, labels)\n",
        "    return accuracy_and_counts\n",
        "\n",
        "def evaluate(weights, biases, val_loader, epoch=None):\n",
        "    accum_acc_and_counts = [batch_validation(weights, biases, batch) for batch in val_loader]\n",
        "    return epoch_end_validation(accum_acc_and_counts, epoch=None)\n",
        "\n",
        "def epoch_end_validation(accum_acc_and_counts, epoch):\n",
        "    accum_acc_and_counts = torch.stack(accum_acc_and_counts)\n",
        "    acc, count, total = accum_acc_and_counts[:, 0].mean(), accum_acc_and_counts[:, 1].sum(), accum_acc_and_counts[:, 2].sum()\n",
        "    print(f\"Epoch {epoch}, Accuracy: {acc}, Count: {count}, total: {total}\")\n",
        "\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    count = torch.sum(outputs == labels).item()\n",
        "    return torch.tensor([count/labels.numel(), count, labels.numel()])\n",
        "\n",
        "\n",
        "def train(num_epochs, learning_rate):\n",
        "    num_features = 784\n",
        "    num_neurons = 10\n",
        "\n",
        "    weights = [[random.random() for _ in range(num_features)] for _ in range(num_neurons)]\n",
        "    biases = [random.random() for _ in range(num_neurons)]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            input_batch =input_batch.squeeze().tolist()\n",
        "            input_batch = [[item for sublist in outer for item in sublist] for outer in input_batch]\n",
        "            target_batch = target_batch.tolist()\n",
        "            target_batch = [[l] for l in target_batch]\n",
        "            target_batch = one_hot_encode(target_batch, 10)\n",
        "            batch_output = forward_pass(input_batch, weights, biases)\n",
        "            errors = calculate_errors(batch_output, target_batch)\n",
        "            deltas = calculate_weight_and_bias_deltas(errors, input_batch, learning_rate)\n",
        "            weights, biases = update_weights_biases(deltas, weights, biases)\n",
        "        end = time.time()\n",
        "\n",
        "        if epoch % 2 == 0 or epoch % 2 == 1:\n",
        "            total_error = sum(sum(e) for e in errors[0])\n",
        "            print(f\"Epoch {epoch+1} completed in {end - start} seconds\")\n",
        "            print(f\"Epoch {epoch}, Error: {total_error}, weights: {weights}, Biases: {biases}\")\n",
        "            evaluate(weights, biases, val_loader, epoch)\n",
        "    return weights, biases\n",
        "\n",
        "num_epochs = 5\n",
        "learning_rate = 0.00125\n",
        "\n",
        "\n",
        "weights, biases = train(num_epochs, learning_rate)\n",
        "print(\"Trained weights:\", weights)\n",
        "print(\"Trained biases:\", biases)\n",
        "\n"
      ],
      "metadata": {
        "id": "77QwSeINYR3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def softmax(x):\n",
        "    # Compute the exponential of each element in the input list\n",
        "    exp_x = [math.exp(i) for i in x]\n",
        "    # Sum all the exponential values\n",
        "    sum_exp_x = sum(exp_x)\n",
        "    # Divide each exponential value by the sum of all exponential values\n",
        "    softmax_x = [j / sum_exp_x for j in exp_x]\n",
        "    return softmax_x\n",
        "\n",
        "# Example usage\n",
        "input_list = [2.0, 1.0, 0.1]\n",
        "output_list = softmax(input_list)\n",
        "\n",
        "print(output_list)\n"
      ],
      "metadata": {
        "id": "cr9BIwqcNd1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_ds[4][0].shape, val_ds[4][1])\n",
        "image_bytes = [val_ds[0][0].squeeze().tolist()]\n",
        "flattened_bytes = [item for sublist1 in image_bytes for sublist2 in sublist1 for item in sublist2]\n",
        "\n",
        "#weights =\n",
        "#biases =\n",
        "\n",
        "input = flattened_bytes\n",
        "outputs = [0] * len(weights)  # Initialize outputs with the length of w\n",
        "\n",
        "for index, pair in enumerate(zip(weights, biases)):\n",
        "    for i in range(len(input)):\n",
        "        outputs[index] += input[i] * pair[0][i] + pair[1]\n",
        "\n",
        "print(outputs)\n",
        "soft_maxed_outputs = softmax(outputs)\n",
        "print(soft_maxed_outputs)\n",
        "print(np.argmax(soft_maxed_outputs))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aKw9y8-QXc65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def one_hot_encode(values, num_classes):\n",
        "    one_hot_encoded = []\n",
        "    for value in values:\n",
        "        one_hot = [0] * num_classes\n",
        "        one_hot[value[0]] = 1\n",
        "        one_hot_encoded.append(one_hot)\n",
        "    return one_hot_encoded\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = [math.exp(i) for i in x]\n",
        "    sum_exp_x = sum(exp_x)\n",
        "    softmax_x = [j / sum_exp_x for j in exp_x]\n",
        "    return softmax_x\n",
        "\n",
        "def forward_pass(input_batch, all_neurons_weights, biases):\n",
        "    batch_output = []\n",
        "    for inputs in input_batch:\n",
        "        layer_output = []\n",
        "        for weights, bias in zip(all_neurons_weights, biases):\n",
        "            neuron_output = sum(x * w for x, w in zip(inputs, weights)) + bias\n",
        "            layer_output.append(neuron_output)\n",
        "        batch_output.append(layer_output)\n",
        "    return batch_output\n",
        "\n",
        "def calculate_errors(batch_output_of_forward_pass, target_batch):\n",
        "    squared_errors = []\n",
        "    residual_errors = []\n",
        "    for output, target in zip(batch_output_of_forward_pass, target_batch):\n",
        "        sample_squared_errors = [(o - t) ** 2 for o, t in zip(output, target)]\n",
        "        sample_residual_errors = [(o - t) for o, t in zip(output, target)]\n",
        "        squared_errors.append(sample_squared_errors)\n",
        "        residual_errors.append(sample_residual_errors)\n",
        "    return squared_errors, residual_errors\n",
        "\n",
        "def calculate_weight_and_bias_deltas(output_of_calculate_errors, input_batch, learning_rate):\n",
        "    residual_errors = output_of_calculate_errors[1]\n",
        "    weight_deltas = [[0 for _ in range(len(input_batch[0]))] for _ in range(len(residual_errors[0]))]\n",
        "    bias_deltas = [0 for _ in range(len(residual_errors[0]))]\n",
        "\n",
        "    for residual_error, inputs in zip(residual_errors, input_batch):\n",
        "        for neuron_index in range(len(residual_error)):\n",
        "            for feature_index in range(len(inputs)):\n",
        "                weight_deltas[neuron_index][feature_index] += learning_rate * residual_error[neuron_index] * inputs[feature_index]\n",
        "            bias_deltas[neuron_index] += learning_rate * residual_error[neuron_index]\n",
        "    return weight_deltas, bias_deltas\n",
        "\n",
        "def update_weights_biases(output_of_calculate_weight_and_bias_deltas, weights, biases):\n",
        "    weight_deltas, bias_deltas = output_of_calculate_weight_and_bias_deltas\n",
        "    for i in range(len(weights)):\n",
        "        for j in range(len(weights[i])):\n",
        "            weights[i][j] -= weight_deltas[i][j]\n",
        "        biases[i] -= bias_deltas[i]\n",
        "    return weights, biases\n",
        "\n",
        "def xavier_init(num_features, num_neurons):\n",
        "    limit = math.sqrt(6 / (num_features + num_neurons))\n",
        "    return [[random.uniform(-limit, limit) for _ in range(num_features)] for _ in range(num_neurons)]\n",
        "\n",
        "def train(num_epochs, learning_rate):\n",
        "    num_features = 784\n",
        "    num_neurons = 10\n",
        "\n",
        "    weights = xavier_init(num_features, num_neurons)\n",
        "    biases = [random.uniform(-math.sqrt(6 / (num_features + num_neurons)), math.sqrt(6 / (num_features + num_neurons))) for _ in range(num_neurons)]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            input_batch = input_batch.squeeze().tolist()\n",
        "            input_batch = [[item for sublist in outer for item in sublist] for outer in input_batch]\n",
        "            target_batch = target_batch.tolist()\n",
        "            target_batch = [[l] for l in target_batch]\n",
        "            target_batch = one_hot_encode(target_batch, 10)\n",
        "            batch_output = forward_pass(input_batch, weights, biases)\n",
        "            errors = calculate_errors(batch_output, target_batch)\n",
        "            deltas = calculate_weight_and_bias_deltas(errors, input_batch, learning_rate)\n",
        "            weights, biases = update_weights_biases(deltas, weights, biases)\n",
        "        end = time.time()\n",
        "\n",
        "        if epoch % 2 == 0 or epoch % 2 == 1:\n",
        "            total_error = sum(sum(e) for e in errors[0])\n",
        "            print(f\"Epoch {epoch+1} completed in {end - start} seconds\")\n",
        "            print(f\"Epoch {epoch}, Error: {total_error}, weights: {weights}, Biases: {biases}\")\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "num_epochs = 3\n",
        "learning_rate = 0.00125\n",
        "\n",
        "# Assume train_loader and val_ds are defined and properly set up\n",
        "\n",
        "weights, biases = train(num_epochs, learning_rate)\n",
        "\n"
      ],
      "metadata": {
        "id": "bJyS1BsjzMw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "5Mh5qd1e6eci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bytes_,lab = val_ds[211]\n",
        "plt.imshow(bytes_[0], cmap='gray')\n",
        "print(\"Label: \", lab)\n",
        "squeezed = bytes_[0].squeeze()\n",
        "image_bytes = [bytes_[0].squeeze().tolist()]\n",
        "flattened_bytes = [item for sublist1 in image_bytes for sublist2 in sublist1 for item in sublist2]\n",
        "\n",
        "input = flattened_bytes\n",
        "outputs = [0] * len(weights)\n",
        "\n",
        "for index, pair in enumerate(zip(weights, biases)):\n",
        "    outputs[index] = sum(x * w for x, w in zip(input, pair[0])) + pair[1]\n",
        "\n",
        "#print(outputs)\n",
        "soft_maxed_outputs = softmax(outputs)\n",
        "print(soft_maxed_outputs)\n",
        "print(np.argmax(soft_maxed_outputs))\n"
      ],
      "metadata": {
        "id": "Sga6yLdm3-xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST"
      ],
      "metadata": {
        "id": "gLer7fPTgUNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MNIST(root='./', download=True)\n",
        "len(dataset)\n",
        "test_dataset = MNIST(root='./', train=False)\n",
        "len(test_dataset)\n",
        "dataset[0]"
      ],
      "metadata": {
        "id": "5tGL7aIJgcwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "BcTQnD0chMJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = dataset[10]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label:', label)"
      ],
      "metadata": {
        "id": "JBB8hJ8gh-84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset (images and labels)\n",
        "dataset = MNIST(root='./',\n",
        "                train=True,\n",
        "                transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "l4uhCTWSkggH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_tensor, label = dataset[0]\n",
        "print(img_tensor.shape, label)"
      ],
      "metadata": {
        "id": "sl9Y35U6kuFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(img_tensor[0,10:15,10:15])\n",
        "print(torch.max(img_tensor), torch.min(img_tensor))"
      ],
      "metadata": {
        "id": "MuAEhu8FmMJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the image by passing in the 28x28 matrix\n",
        "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray');"
      ],
      "metadata": {
        "id": "LP6zo_edsX3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
        "len(train_ds), len(val_ds)"
      ],
      "metadata": {
        "id": "hpHmTC7Os_rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ],
      "metadata": {
        "id": "wyqmeFdtvLZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_size = 28*28\n",
        "num_classes = 10\n",
        "\n",
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)\n",
        "print(model.weight.shape)\n",
        "#model.weight"
      ],
      "metadata": {
        "id": "H538PZIx_UNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.bias.shape)\n",
        "model.bias"
      ],
      "metadata": {
        "id": "DJ3xwm8QFx26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    #print(labels)\n",
        "    #print(images.shape)\n",
        "    outputs = model(images.reshape(-1, 784))\n",
        "    print(outputs.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "lUgp42QvF6Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        xb = xb.reshape(-1, 784)\n",
        "        out = self.linear(xb)\n",
        "        return out"
      ],
      "metadata": {
        "id": "SqFSEvNGIJYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MnistModel()"
      ],
      "metadata": {
        "id": "oXKOJmIOJ5Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    #print(labels)\n",
        "    #print(images.shape)\n",
        "    outputs = model(images)\n",
        "\n",
        "    print(outputs)\n",
        "    break"
      ],
      "metadata": {
        "id": "4jUgp223J8ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.linear.weight.shape)\n",
        "model.parameters()"
      ],
      "metadata": {
        "id": "nlzlcRmaKn3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "sMUDyk7iTlzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(outputs[:1].data).item()"
      ],
      "metadata": {
        "id": "rOtZzygOUKbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = F.softmax(outputs, dim=1)"
      ],
      "metadata": {
        "id": "eOzOtu-pUP3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(probs[:1].data).item()"
      ],
      "metadata": {
        "id": "BWJtEFeQUcxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(probs[:1].data)"
      ],
      "metadata": {
        "id": "q5tw-aKDUqRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_prob, label = torch.max(outputs[:1].data, dim=1)\n",
        "max_prob, label"
      ],
      "metadata": {
        "id": "LDfIfkMoXE4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_probs, preds = torch.max(probs, dim=1)\n",
        "print(preds)\n",
        "#print(max_probs)"
      ],
      "metadata": {
        "id": "O0HPv4NgXqeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(labels == preds)/labels.shape[0]"
      ],
      "metadata": {
        "id": "N4Gcx4nKYQep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "metadata": {
        "id": "DHR6JkSocCA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.where(preds == labels)[0].shape[0]/128"
      ],
      "metadata": {
        "id": "JVXAH8N6cLhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExrI-LR_p5ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#batch_validation(weights, biases, next(iter(val_loader)))\n",
        "x = torch.tensor([3,3, 3, 9])\n",
        "y = torch.tensor([3, 3, 4,9])\n",
        "sum(x == y).item()/x.numel()"
      ],
      "metadata": {
        "id": "N3k-TU00nbn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yC133ntkoCcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #torch.tensor([np.argmax(softmax(output)) for output in [[1,2], [4,3]]])\n",
        "stacked_up = torch.stack([torch.tensor([1.0,5,6,8]), torch.tensor([1.0,2,4,8]), torch.tensor([1,2,3,8])])\n",
        "stacked_up[:, 3].sum(dim=0)"
      ],
      "metadata": {
        "id": "k54j1H-EJBBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xyz(x=None):\n",
        "    if x is None:\n",
        "        x = 1\n",
        "    else:\n",
        "        x = x + 100\n",
        "    return x\n",
        "\n",
        "xyz(9)"
      ],
      "metadata": {
        "id": "amHUSBLFkolA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAmPV1tsjdJuBU7Na5WHkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}