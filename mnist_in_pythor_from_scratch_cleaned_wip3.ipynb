{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratulb/pytorch/blob/main/mnist_in_pythor_from_scratch_cleaned_wip3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "# Define a transform to convert the data to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l19KmJV14sJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the training and test datasets\n",
        "train_dataset = torchvision.datasets.MNIST(root='./', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "ko3R62h8FxV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_list(dataset):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for img, label in dataset:\n",
        "        # Convert the tensor image to a list\n",
        "        img_list = img.squeeze().tolist()\n",
        "        images.append(img_list)\n",
        "        labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "train_images, train_labels = convert_to_list(train_dataset)\n",
        "test_images, test_labels = convert_to_list(test_dataset)\n",
        "\n",
        "print(f\"Number of training images: {len(train_images)}\")\n",
        "print(f\"Number of training labels: {len(train_labels)}\")\n",
        "print(f\"Number of test images: {len(test_images)}\")\n",
        "print(f\"Number of test labels: {len(test_labels)}\")\n"
      ],
      "metadata": {
        "id": "Vi0ygacyGSmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_images_flattened = [item for sublist in train_images for item in sublist]\n"
      ],
      "metadata": {
        "id": "iEUPO7FUY7yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[219]"
      ],
      "metadata": {
        "id": "1DyMr1OXim54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(24)\n",
        "\n",
        "# Initialize weights and biases\n",
        "weights = [[random.random() for x in range(784)] for _ in range(10)]\n",
        "biases = [random.random() * 10 for _ in range(10)]\n",
        "\n",
        "# Generate batch of inputs and corresponding targets\n",
        "#inputs_batch = [[1.2, -2.6, 0.3], [0.5, 1.2, -1.3], [2.1, -1.9, 0.7], [0.8, -0.6, 0.2]]\n",
        "inputs_batch = train_images_flattened[:100]\n",
        "#targets_batch = [[-5, 7, 25], [3, -2, 0], [10, 5, -1], [-7, 8, 3]]\n",
        "targets_batch = [[l] for l in train_labels[:100]]\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1\n",
        "\n",
        "def forward(current_batch, current_layers_neurons_weights, neurons_biases):\n",
        "    \"\"\"\n",
        "    Compute the output of the current layer for each sample in the batch.\n",
        "\n",
        "    Args:\n",
        "        current_batch (list): A list of samples in the batch.\n",
        "        current_layers_neurons_weights (list): A list of weights for the neurons in the current layer.\n",
        "        neurons_biases (list): A list of biases for the neurons in the current layer.\n",
        "\n",
        "    Returns:\n",
        "        batch_outputs (list): A list where each entry corresponds to the output of the current layer\n",
        "                                for a sample in the batch. Each entry is a list of neuron outputs.\n",
        "                                If the layer has 3 neurons, each entry will have a length of 3.\n",
        "                                For example, if the current batch contains 1 sample and the layer has 3 neurons,\n",
        "                                the output will be:\n",
        "                                [[output of n1, output of n2, output of n3]]\n",
        "                                If the batch contains 2 samples, the output will be:\n",
        "                                [[output of n1, output of n2, output of n3],\n",
        "                                 [output of n1, output of n2, output of n3]]\n",
        "                                Each inner list represents the neuron outputs for a corresponding sample in the batch.\n",
        "    \"\"\"\n",
        "    batch_outputs = []\n",
        "\n",
        "    for each_sample in current_batch:\n",
        "        this_sample = each_sample\n",
        "        outputs_of_neurons = [0] * 10\n",
        "        for this_neuron, its_weights in enumerate(current_layers_neurons_weights):\n",
        "            bias_of_this_neuron = neurons_biases[this_neuron]\n",
        "            outputs_of_neurons[this_neuron] = bias_of_this_neuron  # Start with the bias\n",
        "            for feature_index, feature in enumerate(this_sample):\n",
        "                outputs_of_neurons[this_neuron] += its_weights[feature_index] * feature\n",
        "        batch_outputs.append(outputs_of_neurons)\n",
        "    return batch_outputs\n",
        "\n",
        "def batched_errors(batch_outputs, batch_targets):\n",
        "    batch_errors = []\n",
        "    for outputs, targets in zip(batch_outputs, batch_targets):\n",
        "        sample_errors = []\n",
        "        for output, target in zip(outputs, targets):\n",
        "            residual = (output - target)\n",
        "            squared_error = residual ** 2\n",
        "            sample_errors.append((squared_error, residual))\n",
        "        batch_errors.append(sample_errors)\n",
        "    return batch_errors\n",
        "\n",
        "def weight_deltas(batch_errors, inputs_batch):\n",
        "    batch_weight_deltas = []\n",
        "    bias_deltas = [0] * len(batch_errors[0])  # Initialize bias deltas to zero\n",
        "    for errors, inputs in zip(batch_errors, inputs_batch):\n",
        "        sample_weight_deltas = []\n",
        "        for neuron, error in enumerate(errors):\n",
        "            weight_delta = [x * error[1] * learning_rate for x in inputs]\n",
        "            sample_weight_deltas.append(weight_delta)\n",
        "            bias_deltas[neuron] += error[1] * learning_rate  # Accumulate bias delta\n",
        "        batch_weight_deltas.append(sample_weight_deltas)\n",
        "    return batch_weight_deltas, bias_deltas\n",
        "\n",
        "def update_weights(weights, biases, batch_weight_deltas, bias_deltas):\n",
        "    for weight_deltas in batch_weight_deltas:\n",
        "        for i in range(len(weights)):\n",
        "            for j in range(len(weights[i])):\n",
        "                #weights[i][j] -= weight_deltas[i][j]\n",
        "                continue\n",
        "    for i in range(len(biases)):\n",
        "        biases[i] -= bias_deltas[i] / len(batch_weight_deltas)  # Average the bias updates\n",
        "    return weights, biases\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    batch_preds = forward(inputs_batch, weights, biases)\n",
        "    batch_errors_ = batched_errors(batch_preds, targets_batch)\n",
        "    batch_weight_deltas, bias_deltas = weight_deltas(batch_errors_, inputs_batch)\n",
        "    weights, biases = update_weights(weights, biases, batch_weight_deltas, bias_deltas)\n",
        "\n",
        "print(\"Updated weights:\", weights)\n",
        "print(\"Updated biases:\", biases)\n"
      ],
      "metadata": {
        "id": "xT-yzmzgDVfS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "d74f9810-e81a-417b-c823-0636216e2d4b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-119fc22a73ad>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mbatch_errors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mbatch_weight_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_errors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_weight_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_deltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Updated weights:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-106-119fc22a73ad>\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(weights, biases, batch_weight_deltas, bias_deltas)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mbias_deltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_weight_deltas\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Average the bias updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[[random.random() for x in range(4)] for _ in range(3)]\n",
        "len(biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXjuH0pkzqKf",
        "outputId": "764210a8-1d7c-4cfe-b5e7-0e4f3eecf390"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/6B5GnC/ee7OEEy/jL6vS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}