{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratulb/pytorch/blob/main/mnist_in_pythor_from_scratch_cleaned_wip4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import time\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "# Define a transform to convert the data to tensor\n",
        "random.seed(24)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(24)\n",
        "\n",
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "# Load the MNIST dataset"
      ],
      "metadata": {
        "id": "l19KmJV14sJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the training and test datasets\n",
        "train_dataset = torchvision.datasets.MNIST(root='./', train=True, download=True, transform=to_tensor)\n",
        "#test_dataset = torchvision.datasets.MNIST(root='./', train=False, download=True, transform=to_tensor)\n",
        "train_ds, val_ds = random_split(train_dataset, [59000, 1000])\n",
        "len(train_ds), len(val_ds)\n",
        "#len(train_dataset)"
      ],
      "metadata": {
        "id": "ko3R62h8FxV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ],
      "metadata": {
        "id": "9vpvDXVpAqxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(values, num_classes):\n",
        "    one_hot_encoded = []\n",
        "    for value in values:\n",
        "        one_hot = [0] * num_classes\n",
        "        one_hot[value[0]] = 1\n",
        "        one_hot_encoded.append(one_hot)\n",
        "    return one_hot_encoded\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = [math.exp(i) for i in x]\n",
        "    sum_exp_x = sum(exp_x)\n",
        "    softmax_x = [j / sum_exp_x for j in exp_x]\n",
        "    return softmax_x\n",
        "\n",
        "def relu(x):\n",
        "    return max(0, x)\n"
      ],
      "metadata": {
        "id": "xT-yzmzgDVfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(input_batch, all_neurons_weights, biases):\n",
        "    batch_output = []\n",
        "    for inputs in input_batch:\n",
        "        layer_output = []\n",
        "        for weights, bias in zip(all_neurons_weights, biases):\n",
        "            neuron_output = relu(sum(x * w for x, w in zip(inputs, weights)) + bias)\n",
        "            layer_output.append(neuron_output)\n",
        "        batch_output.append(layer_output)\n",
        "    return batch_output\n",
        "\n",
        "def calculate_errors(batch_output_of_forward_pass, target_batch):\n",
        "    squared_errors = []\n",
        "    residual_errors = []\n",
        "    for output, target in zip(batch_output_of_forward_pass, target_batch):\n",
        "        try:\n",
        "            sample_squared_errors = [(o - t) ** 2 for o, t in zip(output, target)]\n",
        "        except OverflowError as e:\n",
        "            print(\"Overflow error.\", e)\n",
        "            sample_squared_errors = 0\n",
        "        #sample_squared_errors = [(o - t) ** 2 for o, t in zip(output, target)]\n",
        "        sample_residual_errors = [(o - t) for o, t in zip(output, target)]\n",
        "        squared_errors.append(sample_squared_errors)\n",
        "        residual_errors.append(sample_residual_errors)\n",
        "    return squared_errors, residual_errors\n",
        "\n",
        "def calculate_weight_and_bias_deltas(output_of_calculate_errors, input_batch, learning_rate):\n",
        "    residual_errors = output_of_calculate_errors[1]\n",
        "    weight_deltas = [[0 for _ in range(len(input_batch[0]))] for _ in range(len(residual_errors[0]))]\n",
        "    bias_deltas = [0 for _ in range(len(residual_errors[0]))]\n",
        "\n",
        "    for residual_error, inputs in zip(residual_errors, input_batch):\n",
        "        for neuron_index in range(len(residual_error)):\n",
        "            for feature_index in range(len(inputs)):\n",
        "                weight_deltas[neuron_index][feature_index] += learning_rate * residual_error[neuron_index] * inputs[feature_index]\n",
        "            bias_deltas[neuron_index] += learning_rate * residual_error[neuron_index]\n",
        "    return weight_deltas, bias_deltas\n",
        "\n",
        "def update_weights_biases(output_of_calculate_weight_and_bias_deltas, weights, biases):\n",
        "    weight_deltas, bias_deltas = output_of_calculate_weight_and_bias_deltas\n",
        "    for i in range(len(weights)):\n",
        "        for j in range(len(weights[i])):\n",
        "            weights[i][j] -= weight_deltas[i][j]\n",
        "        biases[i] -= bias_deltas[i]\n",
        "    return weights, biases\n",
        "\n",
        "def batch_validation(weights, biases, batch):\n",
        "    images, labels = batch\n",
        "    images = [image.squeeze().tolist() for image in images]\n",
        "    images = [[pixel for row in image for pixel in row] for image in images]\n",
        "    outputs = forward_pass(images, weights, biases)\n",
        "    outputs = torch.tensor([np.argmax(softmax(output)) for output in outputs])\n",
        "    accuracy_and_counts = accuracy(outputs, labels)\n",
        "    return accuracy_and_counts\n",
        "\n",
        "def evaluate(weights, biases, val_loader, epoch=None):\n",
        "    accum_acc_and_counts = [batch_validation(weights, biases, batch) for batch in val_loader]\n",
        "    return epoch_end_validation(accum_acc_and_counts, epoch=epoch)\n",
        "\n",
        "def epoch_end_validation(accum_acc_and_counts, epoch):\n",
        "    accum_acc_and_counts = torch.stack(accum_acc_and_counts)\n",
        "    acc, count, total = accum_acc_and_counts[:, 0].mean(), accum_acc_and_counts[:, 1].sum(), accum_acc_and_counts[:, 2].sum()\n",
        "    print(f\"Epoch {epoch}, Accuracy: {acc}, Count: {count}, total: {total}\")\n",
        "\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    count = torch.sum(outputs == labels).item()\n",
        "    return torch.tensor([count/labels.numel(), count, labels.numel()])\n",
        "\n",
        "def xavier_init(num_features, num_neurons):\n",
        "    limit = math.sqrt(6 / (num_features + num_neurons))\n",
        "    return [[random.uniform(-limit, limit) for _ in range(num_features)] for _ in range(num_neurons)]\n",
        "\n",
        "def reformat_batch(input_batch, target_batch):\n",
        "    input_batch = input_batch.squeeze().tolist()\n",
        "    input_batch = [[item for sublist in outer for item in sublist] for outer in input_batch]\n",
        "    target_batch = target_batch.tolist()\n",
        "    target_batch = [[l] for l in target_batch]\n",
        "    target_batch = one_hot_encode(target_batch, 10)\n",
        "    return input_batch, target_batch\n",
        "\n",
        "def train(num_epochs, learning_rate):\n",
        "    num_features = 784\n",
        "    num_neurons = 10\n",
        "\n",
        "    weights = xavier_init(num_features, num_neurons)\n",
        "    biases = [random.uniform(-math.sqrt(6 / (num_features + num_neurons)), math.sqrt(6 / (num_features + num_neurons))) for _ in range(num_neurons)]\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            input_batch, target_batch = reformat_batch(input_batch, target_batch)\n",
        "            batch_output = forward_pass(input_batch, weights, biases)\n",
        "            errors = calculate_errors(batch_output, target_batch)\n",
        "            deltas = calculate_weight_and_bias_deltas(errors, input_batch, learning_rate)\n",
        "            weights, biases = update_weights_biases(deltas, weights, biases)\n",
        "        end = time.time()\n",
        "\n",
        "        if epoch % 2 == 0 or epoch % 2 == 1:\n",
        "            total_error = sum(sum(e) for e in errors[0])\n",
        "            print(f\"Epoch {epoch+1} completed in {end - start} seconds\")\n",
        "            print(f\"Epoch {epoch+1}, Error: {total_error}, weights: {weights}, Biases: {biases}\")\n",
        "            evaluate(weights, biases, val_loader, epoch+1)\n",
        "    return weights, biases\n",
        "\n",
        "num_epochs = 1\n",
        "learning_rate = 0.00125\n",
        "activator = nn.Tanh()\n",
        "\n",
        "\n",
        "weights, biases = train(num_epochs, learning_rate)\n",
        "print(\"Trained weights:\", weights)\n",
        "print(\"Trained biases:\", biases)\n",
        "\n"
      ],
      "metadata": {
        "id": "77QwSeINYR3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = torchvision.datasets.MNIST(root='./', train=False, download=True, transform=to_tensor)"
      ],
      "metadata": {
        "id": "Wgapv5_NyCjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=10)"
      ],
      "metadata": {
        "id": "z9h2PY-ayRu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(weights, biases, test_loader, epoch=None)"
      ],
      "metadata": {
        "id": "MCgpIyHPyjue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu2deriv(output):\n",
        "    return output > 0"
      ],
      "metadata": {
        "id": "PSImxgIkRot_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu2deriv(-1)"
      ],
      "metadata": {
        "id": "MAwbyS-gR0PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "def relu(x):\n",
        " return (x > 0) * x\n",
        "def relu2deriv(output):\n",
        " return output>0\n",
        "streetlights = np.array( [[ 1, 0, 1 ],\n",
        " [ 0, 1, 1 ],\n",
        " [ 0, 0, 1 ],\n",
        " [ 1, 1, 1 ] ] )\n",
        "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T\n",
        "\n",
        "alpha = 0.2\n",
        "hidden_size = 4\n",
        "weights_0_1 = 2*np.random.random((3,hidden_size)) - 1\n",
        "weights_1_2 = 2*np.random.random((hidden_size,1)) - 1\n",
        "for iteration in range(60):\n",
        " layer_2_error = 0\n",
        " for i in range(len(streetlights)):\n",
        "    layer_0 = streetlights[i:i+1]\n",
        "    layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
        "    layer_2 = np.dot(layer_1,weights_1_2)\n",
        "    layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)\n",
        "    layer_2_delta = (layer_2 - walk_vs_stop[i:i+1])\n",
        "    layer_1_delta=layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)\n",
        "    weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
        "    weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
        "    if(iteration % 10 == 9):\n",
        "        print(\"Error:\" + str(layer_2_error))"
      ],
      "metadata": {
        "id": "YuwjRPWySUJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "100 * True\n"
      ],
      "metadata": {
        "id": "JB1sijQHS7fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "weights = np.array([0.5,0.48,-0.7])\n",
        "alpha = 0.001\n",
        "streetlights = np.array( [ [ 1, 0, 1 ],\n",
        " [ 0, 1, 1 ],\n",
        " [ 0, 0, 1 ],\n",
        " [ 1, 1, 1 ],\n",
        " [ 0, 1, 1 ],\n",
        " [ 1, 0, 1 ] ] )\n",
        "walk_vs_stop = np.array( [ 0, 1, 0, 1, 1, 0 ] )\n",
        "input = streetlights[0]\n",
        "goal_prediction = walk_vs_stop[0]\n",
        "for iteration in range(5000):\n",
        "    prediction = input.dot(weights)\n",
        "    error = (goal_prediction - prediction) ** 2\n",
        "    delta = prediction - goal_prediction\n",
        "    weights = weights - (alpha * (input * delta))\n",
        "    print(\"Error:\" + str(error) + \" Prediction:\" + str(prediction))"
      ],
      "metadata": {
        "id": "1VyKwp7lf5H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.array([0.5,0.48,-0.7])\n",
        "alpha = 0.1\n",
        "streetlights = np.array( [[ 1, 0, 1 ],\n",
        " [ 0, 1, 1 ],\n",
        " [ 0, 0, 1 ],\n",
        " [ 1, 1, 1 ],\n",
        " [ 0, 1, 1 ],\n",
        " [ 1, 0, 1 ] ] )\n",
        "walk_vs_stop = np.array( [ 0, 1, 0, 1, 1, 0 ] )\n",
        "#input = streetlights[0]\n",
        "#goal_prediction = walk_vs_stop[0]\n",
        "for iteration in range(40):\n",
        " error_for_all_lights = 0\n",
        " for row_index in range(len(walk_vs_stop)):\n",
        "    input = streetlights[row_index]\n",
        "    goal_prediction = walk_vs_stop[row_index]\n",
        "\n",
        "    prediction = input.dot(weights)\n",
        "\n",
        "    error = (goal_prediction - prediction) ** 2\n",
        "    error_for_all_lights += error\n",
        "\n",
        "    delta = prediction - goal_prediction\n",
        "    weights = weights - (alpha * (input * delta))\n",
        "    print(\"Prediction:\" + str(prediction))\n",
        "    print(\"Error:\" + str(error_for_all_lights) + \"\\n\")"
      ],
      "metadata": {
        "id": "fnXwcXWAigW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([1,1,1]).dot(weights)\n",
        "weights"
      ],
      "metadata": {
        "id": "SmIfQ37Rj3RJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCDaZNhPLdTwtV887pEnDm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}