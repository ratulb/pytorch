{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratulb/pytorch/blob/main/numy_regression_two_variables_scalar_mul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tasXNgdCAzh"
      },
      "source": [
        "****This notebook implements a numpy based linear regression based on two variables(based only on numpy)****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnBsRpb4HV8U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x = tf.constant([[1., 2., 3.],\n",
        "                 [4., 5., 6.]])\n",
        "\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "id": "mpMcqG9nePq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a 1D tensor\n",
        "tensor_1d = torch.tensor([1, 2, 3, 4])\n",
        "\n",
        "print(\"Original tensor:\", tensor_1d)\n",
        "print(\"Shape of original tensor:\", tensor_1d.shape)\n",
        "\n",
        "# Unsqueeze the tensor at dimension 0\n",
        "tensor_unsqueezed_0 = torch.unsqueeze(tensor_1d, 0)\n",
        "print(\"\\nTensor after unsqueezing at dim 0:\", tensor_unsqueezed_0)\n",
        "print(\"Shape of tensor after unsqueezing at dim 0:\", tensor_unsqueezed_0.shape)\n",
        "\n",
        "# Unsqueeze the tensor at dimension 1\n",
        "tensor_unsqueezed_1 = torch.unsqueeze(tensor_1d, 1)\n",
        "print(\"\\nTensor after unsqueezing at dim 1:\", tensor_unsqueezed_1)\n",
        "print(\"Shape of tensor after unsqueezing at dim 1:\", tensor_unsqueezed_1.shape)\n"
      ],
      "metadata": {
        "id": "SlxEss1_hgue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 2D tensor\n",
        "tensor_2d = torch.tensor([[1, 2], [3, 4]])\n",
        "\n",
        "print(\"Original tensor:\", tensor_2d)\n",
        "print(\"Shape of original tensor:\", tensor_2d.shape)\n",
        "\n",
        "# Unsqueeze the tensor at dimension 0\n",
        "tensor_unsqueezed_0 = torch.unsqueeze(tensor_2d, 0)\n",
        "print(\"\\nTensor after unsqueezing at dim 0:\", tensor_unsqueezed_0)\n",
        "print(\"Shape of tensor after unsqueezing at dim 0:\", tensor_unsqueezed_0.shape)\n",
        "\n",
        "# Unsqueeze the tensor at dimension 1\n",
        "tensor_unsqueezed_1 = torch.unsqueeze(tensor_2d, 1)\n",
        "print(\"\\nTensor after unsqueezing at dim 1:\", tensor_unsqueezed_1)\n",
        "print(\"Shape of tensor after unsqueezing at dim 1:\", tensor_unsqueezed_1.shape)\n",
        "\n",
        "# Unsqueeze the tensor at dimension 2\n",
        "tensor_unsqueezed_2 = torch.unsqueeze(tensor_2d, 2)\n",
        "print(\"\\nTensor after unsqueezing at dim 2:\", tensor_unsqueezed_2)\n",
        "print(\"Shape of tensor after unsqueezing at dim 2:\", tensor_unsqueezed_2.shape)\n"
      ],
      "metadata": {
        "id": "6oUtWw-NjGQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "string_data = ['a', 'b', 'c']\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_data = label_encoder.fit_transform(string_data)\n",
        "\n",
        "tensor_data = torch.tensor(encoded_data)\n",
        "print(\"Encoded tensor data:\", tensor_data)\n",
        "print(\"Decoded tensor data:\", label_encoder.inverse_transform(tensor_data))\n"
      ],
      "metadata": {
        "id": "BZ2Jr8_vj7S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "string_data = np.array(['axx', 'b', 'c']).reshape(-1, 1)\n",
        "print(\"Original string data:\", string_data)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "onehot_encoded_data = onehot_encoder.fit_transform(string_data)\n",
        "\n",
        "tensor_data = torch.tensor(onehot_encoded_data)\n",
        "print(\"One-hot encoded tensor data:\\n\", tensor_data)\n"
      ],
      "metadata": {
        "id": "DhLnKjdQl0OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_data = torch.tensor([[1, 2], [3, 4]]) + torch.tensor([5, 6])\n",
        "print(\"Original tensor data:\\n\", tensor_data)"
      ],
      "metadata": {
        "id": "w9_3DEoHqMRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tf.config.list_physical_devices('GPU'):\n",
        "  print(\"TensorFlow **IS** using the GPU\")\n",
        "else:\n",
        "  print(\"TensorFlow **IS NOT** using the GPU\")"
      ],
      "metadata": {
        "id": "jVjBDGDlenf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def poly_desc(W, b):\n",
        "    \"\"\"Creates a string description of a polynomial.\"\"\"\n",
        "    result = 'y = '\n",
        "    for i, w in enumerate(W):\n",
        "        result += '{:+.2f} x^{} '.format(w, i + 1)\n",
        "    result += '{:+.2f}'.format(b[0])\n",
        "    return result\n",
        "\n",
        "res = poly_desc([1, 3, 0.9], [2])\n",
        "print(res)"
      ],
      "metadata": {
        "id": "JBMqISfVT5sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JCMmbmNgqwV8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "N = 5\n",
        "idx = np.arange(N)\n",
        "np.random.shuffle(idx)\n",
        "print(idx)\n",
        "train_idx = idx[:int(N*.8)]\n",
        "\n",
        "\n",
        "#Generate data\n",
        "np.random.seed(42)\n",
        "inputs, targets, coef = make_regression(n_samples=100, n_features=3, n_informative=2, noise=5, coef=True,\n",
        "                                        bias=7,  random_state=42)\n",
        "\n",
        "inputs = np.column_stack((inputs, np.ones(len(inputs))))\n",
        "targets = targets.reshape(-1, 1)\n",
        "weights = np.random.randn(inputs.shape[1],1)\n",
        "print(inputs.shape, targets.shape, weights.shape, coef)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjbbIjmEvkfe"
      },
      "outputs": [],
      "source": [
        "#Train\n",
        "num_epochs = 10000\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2Y4g5QTvu-F"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    for epoch in range(num_epochs):\n",
        "        global weights\n",
        "        predictions = inputs @ weights\n",
        "        error = targets - predictions\n",
        "        #print(error.shape)\n",
        "        loss = (error ** 2).mean()\n",
        "        if (epoch + 1) % 200 == 0:\n",
        "            print(f\"Epoch: {epoch + 1}, Loss: {loss}\")\n",
        "\n",
        "        gradients = (-2 * inputs.T @ error) / len(inputs)\n",
        "        weights -= learning_rate * gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMyt9wqDwjt0"
      },
      "outputs": [],
      "source": [
        "train()\n",
        "print(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-phTYaGaQQa"
      },
      "source": [
        "****Generate data points and visualize****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiccHHH8RS3q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data generation\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "inputs = np.random.randn(1000, 10)\n",
        "targets = (inputs[:, 0] + inputs[:, 1] * 2 + np.random.randn(1000)).reshape(-1, 1)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Define the model\n",
        "class TabularModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(TabularModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = TabularModel(input_dim=X_train.shape[1])\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        model.eval()\n",
        "        val_outputs = model(X_val)\n",
        "        val_loss = criterion(val_outputs, y_val)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "# Final evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_outputs = model(X_val)\n",
        "    val_loss = criterion(val_outputs, y_val)\n",
        "    print(f'Final Validation Loss: {val_loss.item():.4f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHWEvScjoJ8p"
      },
      "outputs": [],
      "source": [
        "# Create a sample user-item interaction matrix (with some missing values)\n",
        "R = np.array([\n",
        "    [5, 3, 0, 1],\n",
        "    [4, 0, 0, 1],\n",
        "    [1, 1, 0, 5],\n",
        "    [1, 0, 0, 4],\n",
        "    [0, 1, 5, 4],\n",
        "])\n",
        "\n",
        "num_users, num_items = R.shape\n",
        "num_factors = 2  # Number of latent factors\n",
        "\n",
        "# Initialize user and item latent factor matrices\n",
        "U = np.random.rand(num_users, num_factors)\n",
        "V = np.random.rand(num_items, num_factors)\n",
        "\n",
        "# Learning parameters\n",
        "learning_rate = 0.01\n",
        "num_epochs = 5000\n",
        "lambda_reg = 0.01  # Regularization parameter\n",
        "\n",
        "# Mask for observed entries\n",
        "observed = R > 0\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(num_users):\n",
        "        for j in range(num_items):\n",
        "            if observed[i, j]:\n",
        "                error_ij = R[i, j] - np.dot(U[i, :], V[j, :])\n",
        "                for k in range(num_factors):\n",
        "                    U[i, k] += learning_rate * (error_ij * V[j, k] - lambda_reg * U[i, k])\n",
        "                    V[j, k] += learning_rate * (error_ij * U[i, k] - lambda_reg * V[j, k])\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        predicted_R = U.dot(V.T)\n",
        "        error = np.sum((observed * (R - predicted_R)) ** 2)\n",
        "        print(f'Epoch: {epoch + 1}, Error: {error}')\n",
        "\n",
        "# Final prediction\n",
        "predicted_R = U.dot(V.T)\n",
        "print(predicted_R)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lME3H8ICorLo"
      },
      "outputs": [],
      "source": [
        "from fastai.collab import *\n",
        "ratings = pd.read_csv (untar_data(URLs.ML_SAMPLE) /'ratings.csv' )\n",
        "dls = CollabDataLoaders .from_df (ratings,bs=64 , seed=42 )\n",
        "learn = collab_learner ( dls , n_factors =50 , y_range = [ 0 , 5.5 ] )\n",
        "learn.fit_one_cycle(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX1ya_UYdRA2"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "path = untar_data(URLs.PETS)/'images'\n",
        "\n",
        "def is_cat(x): return x[0].isupper()\n",
        "dls = ImageDataLoaders.from_name_func(\n",
        "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
        "    label_func=is_cat, item_tfms=Resize(224))\n",
        "\n",
        "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
        "learn.fine_tune(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK-mPsS8fOum"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMtyYxB8iBEw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# URL of the image\n",
        "image_path = '/content/800px-A-Cat.jpg'\n",
        "#image_path = '/content/dog.png'\n",
        "\n",
        "try:\n",
        "    # Fetch the image\n",
        "\n",
        "    # Open the image using PIL\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Display the image using PIL\n",
        "    #img.show()\n",
        "\n",
        "    # Optionally, display the image using matplotlib\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')  # Hide the axis\n",
        "    #plt.show()\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DQ4a4nUp7qe"
      },
      "outputs": [],
      "source": [
        "is_cat,_,probs = learn.predict(img)\n",
        "print(f\"Is this a cat?: {is_cat}.\")\n",
        "print(f\"Probability it's a cat: {probs[1].item():.6f}\")\n",
        "print(learn.epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31KShYVLLGkL"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "path = untar_data(URLs.CAMVID_TINY)\n",
        "dls = SegmentationDataLoaders.from_label_func(\n",
        "    path, bs=8, fnames = get_image_files(path/\"images\"),\n",
        "    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n",
        "    codes = np.loadtxt(path/'codes.txt', dtype=str)\n",
        ")\n",
        "\n",
        "learn = unet_learner(dls, resnet18)\n",
        "learn.fine_tune(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0KWGDWzLzc5"
      },
      "outputs": [],
      "source": [
        "learn.show_results(max_n=6, figsize=(7,8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5kr62S-MQAY"
      },
      "outputs": [],
      "source": [
        "from fastai.text.all import *\n",
        "\n",
        "dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n",
        "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn.fine_tune(4, 1e-2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J78hMD5lVmo6"
      },
      "outputs": [],
      "source": [
        "learn.predict(\"I really liked that movie!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQEFtNzfVxzp"
      },
      "outputs": [],
      "source": [
        "3.7999e-04 + 9.9962e-01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvbbXrfvV6N5"
      },
      "outputs": [],
      "source": [
        "learn.predict(\"I just love how the new update made my favorite app so much worse. Now I can't even use it without crashing every five minutes. Thanks for ruining my day, developers!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gpys-p_Wg8C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer for sentiment analysis and sarcasm detection\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Review text\n",
        "review = \"I just love how the new update made my favorite app so much worse. Now I can't even use it without crashing every five minutes. Thanks for ruining my day, developers! #BestUpdateEver\"\n",
        "\n",
        "# Perform sentiment analysis\n",
        "results = sentiment_pipeline(review)\n",
        "print(results)\n",
        "\n",
        "# Additional sarcasm detection (hypothetical, as specialized sarcasm detection models can be implemented similarly)\n",
        "# sarcasm_model_name = \"sarcasm-detection-model\"\n",
        "# sarcasm_tokenizer = AutoTokenizer.from_pretrained(sarcasm_model_name)\n",
        "# sarcasm_model = AutoModelForSequenceClassification.from_pretrained(sarcasm_model_name)\n",
        "# sarcasm_pipeline = pipeline(\"sentiment-analysis\", model=sarcasm_model, tokenizer=sarcasm_tokenizer)\n",
        "# sarcasm_results = sarcasm_pipeline(review)\n",
        "# print(sarcasm_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfKYGN5YXBUL"
      },
      "outputs": [],
      "source": [
        "doc(learn.predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibtjs_FXXjeR"
      },
      "outputs": [],
      "source": [
        "from fastai.tabular.all import *\n",
        "path = untar_data(URLs.ADULT_SAMPLE)\n",
        "\n",
        "dls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n",
        "    cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n",
        "                 'relationship', 'race'],\n",
        "    cont_names = ['age', 'fnlwgt', 'education-num'],\n",
        "    procs = [Categorify, FillMissing, Normalize])\n",
        "\n",
        "learn = tabular_learner(dls, metrics=accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cJgneOfX81K"
      },
      "outputs": [],
      "source": [
        "learn.fit_one_cycle(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggmnpWo0YWQp"
      },
      "outputs": [],
      "source": [
        "from fastai.collab import *\n",
        "path = untar_data(URLs.ML_SAMPLE)\n",
        "dls = CollabDataLoaders.from_csv(path/'ratings.csv')\n",
        "learn = collab_learner(dls, y_range=(0.5,5.5))\n",
        "learn.fine_tune(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsOW3FJJZCo6"
      },
      "outputs": [],
      "source": [
        "learn.show_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olyuRmGqHjet"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "inputs, targets, coef = make_regression(n_samples=100, n_features=2, n_informative=2, noise=5, coef=True,\n",
        "                                        bias=0.75,  random_state=2)\n",
        "\n",
        "print(inputs.shape)\n",
        "print(targets.shape)\n",
        "print(coef)\n",
        "x = inputs[:, 0]\n",
        "y = inputs[:, 1]\n",
        "x, y = np.meshgrid(x, y)\n",
        "z = coef[0]*x + coef[1]*y + 0.75 #bias\n",
        "\n",
        "# Create a figure and a 3D axis\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot the surface\n",
        "surface = ax.plot_surface(x, y, z, cmap='viridis')\n",
        "\n",
        "# Add labels\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "# Add a color bar which maps values to colors\n",
        "fig.colorbar(surface)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z3cTzNrZNo6"
      },
      "source": [
        "****Split the data points into train, validation and test sets****\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyha18v5aEYi"
      },
      "outputs": [],
      "source": [
        "\n",
        "#80% of inputs for train and validation and rest 20% for tests\n",
        "inputs_train_val, inputs_test, targets_train_val, targets_test = train_test_split(\n",
        "    inputs, targets, test_size=0.2, random_state=42\n",
        ")\n",
        "#75% of inputs_train_val into training inputs and 25% for train validation\n",
        "inputs_train, inputs_val, targets_train, targets_val = train_test_split(\n",
        "    inputs_train_val, targets_train_val, test_size=0.25, random_state=42\n",
        ")\n",
        "print(inputs_train.shape)\n",
        "\n",
        "print(targets_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqhfTZg7tdmR"
      },
      "source": [
        "****Weights and biases****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd83vVtwkm_J"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10000\n",
        "learning_rate = 0.001\n",
        "w = np.random.randn(2, 1)\n",
        "b = np.random.randn(1)\n",
        "print(w)\n",
        "print(w.T)\n",
        "print(b)\n",
        "targets_train = targets_train.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV5Euj0MtbJB"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    yhat = inputs_train @ w + b\n",
        "    print(inputs_train.shape)\n",
        "    print(w.shape)\n",
        "    print(b.shape)\n",
        "    print(yhat.shape)\n",
        "\n",
        "    error = targets_train - yhat\n",
        "    print(error.shape)\n",
        "\n",
        "    loss = (error ** 2).mean()\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch: {epoch + 1}, Loss: {loss}\")\n",
        "\n",
        "    w_grad = (-2 * inputs_train.T @ error) / len(inputs_train)\n",
        "    b_grad = (-2 * error).mean(axis=0)\n",
        "\n",
        "    # Update weights\n",
        "    w -= learning_rate * w_grad\n",
        "    b -= learning_rate * b_grad\n",
        "\n",
        "\n",
        "print(f\"w: {w}, b: {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XnHl7BA5Tg"
      },
      "source": [
        "**Very that weights/bias calculated also agree with returned coeffient and that we obtain from sklearn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bqx5sqxzArB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(coef)\n",
        "\n",
        "linr = LinearRegression()\n",
        "linr.fit(inputs_train, targets_train)\n",
        "print(\"\\nWeights and bias from sklearn\\n\")\n",
        "print(linr.intercept_, linr.coef_[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLFaZ3bRgsg6"
      },
      "outputs": [],
      "source": [
        "# Generate some sample data\n",
        "np.random.seed(0)\n",
        "n_samples = 100\n",
        "x = np.random.rand(n_samples)\n",
        "y = np.random.rand(n_samples)\n",
        "targets = 3.0 * x + 2.0 * y + 1.0 + 0.5 * np.random.randn(n_samples)  # Generating target values with some noise\n",
        "\n",
        "# Stack x, y and a column of ones to create the input matrix\n",
        "X = np.column_stack((x, y, np.ones_like(x)))\n",
        "print(X.shape)\n",
        "print(X[:3])\n",
        "\n",
        "# Initialize weights\n",
        "weights = np.random.randn(3)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    z_pred = X @ weights\n",
        "    error = targets - z_pred\n",
        "    print(\"here\")\n",
        "    print(error.shape)\n",
        "    print(error[0:3])\n",
        "    break\n",
        "    loss = (error ** 2).mean()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch: {epoch + 1}, Loss: {loss}\")\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = -2 * X.T @ error / len(x)\n",
        "\n",
        "    # Update weights\n",
        "    weights -= learning_rate * gradients\n",
        "\n",
        "print(\"Trained weights:\", weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHnTnO3Mlo4F"
      },
      "outputs": [],
      "source": [
        " x = np.array([1, 2, 3, 4, 5])\n",
        " x =torch.from_numpy(x)\n",
        " torch.cat([x ** i for i in range(1, 5)], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFMs3jOBmXhb"
      },
      "outputs": [],
      "source": [
        "dummy = ['a', 'b', 'c']\n",
        "print(dummy[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZzzYQ_4oxts"
      },
      "outputs": [],
      "source": [
        "# Install TensorBoard\n",
        "!pip install tensorboard\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Generate dummy data\n",
        "inputs = torch.randn(100, 10)\n",
        "targets = torch.randn(100, 1)\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, optimizer, and TensorBoard writer\n",
        "model = SimpleModel()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "writer = SummaryWriter('runs/simple_model_experiment_1')\n",
        "\n",
        "# Training loop with TensorBoard logging\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for xb, yb in dataloader:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(xb)\n",
        "        loss = loss_fn(outputs, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        writer.add_scalar('Training Loss', loss.item(), epoch)\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            writer.add_histogram(name, param, epoch)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "writer.close()\n",
        "\n",
        "# Load TensorBoard in Colab\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZGwnm2uqZrj"
      },
      "outputs": [],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEm2aYe5rI5t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "v = torch.tensor(1.0, requires_grad=True)\n",
        "make_dot(v)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcltKTTbH3cX/e/Mf7I29Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}